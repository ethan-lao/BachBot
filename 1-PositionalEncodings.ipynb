{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVtNSBCCVHDX"
      },
      "source": [
        "# Dataset and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myWtI3bwxc6c",
        "outputId": "5b620a18-4e56-470c-b703-2edeea9f40d3"
      },
      "outputs": [],
      "source": [
        "# get chorale dataset\n",
        "# https://github.com/ageron/handson-ml2/blob/master/datasets/jsb_chorales/README.md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "device='mps' if torch.backends.mps.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2i9LrwDc0Btv",
        "outputId": "c7ebe1fe-4473-4d30-9bfa-ad115f465fde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[36, 81]\n"
          ]
        }
      ],
      "source": [
        "# preprocess the chorales\n",
        "import os\n",
        "import csv\n",
        "\n",
        "note_range = [88, 0]\n",
        "\n",
        "def parse_score_dir(folder_path):\n",
        "    scores = []\n",
        "\n",
        "    # Loop through all files in the directory\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.csv'):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            #print(f\"Contents of {filename}:\")\n",
        "\n",
        "            # Open the CSV file\n",
        "            with open(file_path, 'r') as csv_file:\n",
        "                csv_reader = csv.reader(csv_file)\n",
        "                next(csv_reader) # skip header\n",
        "\n",
        "                voices = [[] for _ in range(4)]\n",
        "                for row in csv_reader:\n",
        "                    for i in range(4):\n",
        "                        voices[i].append(int(row[i]))\n",
        "\n",
        "                        if int(row[i]) != 0:\n",
        "                            note_range[0] = min(note_range[0], int(row[i]))\n",
        "                            note_range[1] = max(note_range[1], int(row[i]))\n",
        "\n",
        "                scores.append([filename, voices])\n",
        "\n",
        "    return scores\n",
        "\n",
        "test = parse_score_dir(\"test\")\n",
        "train = parse_score_dir(\"train\")\n",
        "valid = parse_score_dir(\"valid\")\n",
        "\n",
        "print(note_range)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FIj_FBJEH73e"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from secrets import randbelow\n",
        "import torch\n",
        "\n",
        "def rand_num(min, max_excl):\n",
        "    return randbelow(int(max_excl - min)) + min\n",
        "\n",
        "def split_scores(scores, repeats=5):\n",
        "    res = []\n",
        "    for score in scores:\n",
        "        filename, voices = score\n",
        "        for r in range(repeats):\n",
        "            num_beats = len(voices[0]) / 4\n",
        "            src_seq_length = rand_num(16, 21)\n",
        "\n",
        "            # pick random index for training seq\n",
        "            i = randbelow(int(num_beats - src_seq_length + 1))\n",
        "            src_voices = [v[i*4:(i+src_seq_length)*4] for v in voices]\n",
        "\n",
        "            # pick index for tgt\n",
        "            tgt_voices = None\n",
        "            if i > (num_beats - (i + src_seq_length)):\n",
        "                # tgt at beginning\n",
        "                tgt_seq_length = min(20, rand_num(2, i))\n",
        "                tgt_i = randbelow(i - tgt_seq_length + 1)\n",
        "                tgt_voices = [v[tgt_i*4:(tgt_i+tgt_seq_length)*4] for v in voices]\n",
        "            else:\n",
        "                # tgt at end\n",
        "                tgt_seq_length = min(20, rand_num(2, (num_beats - (i + src_seq_length))))\n",
        "                tgt_i = rand_num(i + src_seq_length + 1, num_beats - tgt_seq_length + 1)\n",
        "                tgt_voices = [v[tgt_i*4:(tgt_i+tgt_seq_length)*4] for v in voices]\n",
        "\n",
        "            res.append([filename, src_voices, tgt_voices])\n",
        "\n",
        "    return res\n",
        "\n",
        "split_train = split_scores(train)\n",
        "split_test = split_scores(test)\n",
        "split_valid = split_scores(valid)\n",
        "\n",
        "# max_src = 0\n",
        "# max_tgt = 0\n",
        "# for s in split_train:\n",
        "#     max_src = max(len(s[1][0]), max_src)\n",
        "#     max_tgt = max(len(s[2][0]), max_tgt)\n",
        "\n",
        "# print(max_src)\n",
        "# print(max_tgt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7Cfx3in5SRer"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "\n",
        "PAD_TOKEN_ID = 0\n",
        "EOS_TOKEN_ID = 1\n",
        "NO_NOTE_TOKEN_ID = 2\n",
        "NOTE_OFFSET = note_range[0] - 3\n",
        "VOCAB_SIZE = 3 + note_range[1] - note_range[0] + 1\n",
        "\n",
        "def to_dataset(split_scores, max_src_seq_len=324, max_tgt_seq_len=324):\n",
        "    src_data = []\n",
        "    tgt_data = []\n",
        "    for score in split_scores:\n",
        "        filename, src_voices, tgt_voices = score\n",
        "\n",
        "        def note_to_id(note):\n",
        "            if note == 0:\n",
        "                return NO_NOTE_TOKEN_ID\n",
        "            return note - NOTE_OFFSET\n",
        "\n",
        "        def voices_to_src_seq(voices):\n",
        "            seq = torch.zeros(max_src_seq_len, dtype=torch.int64)\n",
        "            for i in range(len(voices[0])):\n",
        "                seq[i*4] = note_to_id(voices[0][i])\n",
        "                seq[i*4 + 1] = note_to_id(voices[1][i])\n",
        "                seq[i*4 + 2] = note_to_id(voices[2][i])\n",
        "                seq[i*4 + 3] = note_to_id(voices[3][i])\n",
        "            return seq\n",
        "\n",
        "        def voices_to_tgt_seq(voices):\n",
        "            seq = torch.zeros(max_tgt_seq_len, dtype=torch.int64)\n",
        "            for i in range(len(voices[0])):\n",
        "                seq[i*4] = note_to_id(voices[0][i])\n",
        "                seq[i*4 + 1] = note_to_id(voices[1][i])\n",
        "                seq[i*4 + 2] = note_to_id(voices[2][i])\n",
        "                seq[i*4 + 3] = note_to_id(voices[3][i])\n",
        "            seq[(i+1)*4] = EOS_TOKEN_ID\n",
        "            return seq\n",
        "\n",
        "        src_data.append(voices_to_src_seq(src_voices))\n",
        "        tgt_data.append(voices_to_tgt_seq(tgt_voices))\n",
        "\n",
        "    dataset = TensorDataset(torch.stack(src_data), torch.stack(tgt_data))\n",
        "    return dataset\n",
        "\n",
        "train_dataset = to_dataset(split_train)\n",
        "test_dataset = to_dataset(split_test)\n",
        "valid_dataset = to_dataset(split_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES2WYCMlVLNw"
      },
      "source": [
        "# Score Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5Vk_jCARyVDo"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from pathlib import Path\n",
        "import musicscore\n",
        "from musicscore import *\n",
        "import uuid\n",
        "\n",
        "gen_id = lambda: f\"a{str(uuid.uuid4()).replace('-', '')}\"\n",
        "\n",
        "def visualize_score(score, name):\n",
        "    title, content = score\n",
        "    newScore = Score(title=title)\n",
        "\n",
        "    soprano = newScore.add_child(Part(gen_id(), name='Soprano'))\n",
        "    alto = newScore.add_child(Part(gen_id(), name='Alto'))\n",
        "    tenor = newScore.add_child(Part(gen_id(), name='Tenor'))\n",
        "    bass = newScore.add_child(Part(gen_id(), name='Bass'))\n",
        "\n",
        "    parts = [soprano, alto, tenor, bass]\n",
        "    clefs = [TrebleClef(), TrebleClef(), BassClef(), BassClef()]\n",
        "\n",
        "    for part_index in range(4):\n",
        "        part_content = content[part_index]\n",
        "        part = parts[part_index]\n",
        "        clef = clefs[part_index]\n",
        "\n",
        "        if (len(part_content) % 16 != 0):\n",
        "            # add pickup measure\n",
        "            pickuptime = musicscore.time.Time()\n",
        "            pickuptime.actual_signatures = [1, 4]\n",
        "\n",
        "            measure = part.add_child(Measure(number=1, time=pickuptime))\n",
        "            staff = measure.add_child(Staff(clef=clef))\n",
        "\n",
        "            normaltime = musicscore.time.Time()\n",
        "            measure = part.add_child(Measure(number=2, time=normaltime))\n",
        "        else:\n",
        "            measure = part.add_child(Measure(number=1))\n",
        "            staff = measure.add_child(Staff(clef=clef))\n",
        "\n",
        "        note_index = 0\n",
        "        while note_index < len(part_content):\n",
        "            note = part_content[note_index]\n",
        "            duration = .25\n",
        "            while (note_index != len(part_content) - 1 and (note_index + 1) % 4 != 0 and note == part_content[note_index + 1]):\n",
        "                duration += .25\n",
        "                note_index += 1\n",
        "\n",
        "            accidental = musicscore.accidental.Accidental(\"sharp\")\n",
        "\n",
        "            #check if next note is descending\n",
        "            check_index = note_index + 1\n",
        "            while True:\n",
        "                if check_index == len(part_content):\n",
        "                    break;\n",
        "\n",
        "                check_note = part_content[check_index]\n",
        "                if (check_note != note):\n",
        "                    if check_note < note and note - check_note < 3:\n",
        "                        accidental.mode = \"flat\"\n",
        "                    break;\n",
        "\n",
        "                check_index += 1\n",
        "\n",
        "\n",
        "            midi = musicscore.midi.Midi(note, accidental)\n",
        "            chord = Chord(midi, duration)\n",
        "            part.add_chord(chord)\n",
        "            note_index += 1\n",
        "\n",
        "    xml_path = Path(name).with_suffix('.xml')\n",
        "    newScore.export_xml(xml_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfMxIh27VeF3"
      },
      "source": [
        "# Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZFhQI7hDVmaW"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000, n=10000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        # pe = torch.zeros(max_len, d_model)\n",
        "        # position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        # div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        # pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        # pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        # pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        # self.register_buffer('pe', pe)\n",
        "\n",
        "\n",
        "\n",
        "        # Generate position encoding based on the given sequence length, hidden size, and frequency (n)\n",
        "\n",
        "        horizontal_pe = torch.zeros(max_len, d_model)\n",
        "\n",
        "        for pos in range(max_len):\n",
        "            for i in range(d_model // 2):\n",
        "                horizontal_pe[pos, 2*i] = torch.sin(torch.tensor(pos / (n**(2*i / d_model))))\n",
        "                horizontal_pe[pos, 2*i+1] = torch.cos(torch.tensor(pos / (n**(2*i / d_model))))\n",
        "        self.register_buffer('horizontal_pe', horizontal_pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(x.shape)\n",
        "        # print(self.horizontal_pe[:x.size(1), :].shape)\n",
        "        return x + self.horizontal_pe[:x.size(1), :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gAC63HS_CUxu"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding2D(nn.Module):\n",
        "    def __init__(self, d_model, max_len=320, n=10000):\n",
        "        super(PositionalEncoding2D, self).__init__()\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "        self.n = n\n",
        "\n",
        "        # Initialize positional encodings\n",
        "        pe = torch.zeros(max_len * 4, d_model)\n",
        "        for col in range(max_len):\n",
        "            for row in range(4):\n",
        "                pos = (row * 4) + col\n",
        "                for i in range(d_model // 2):\n",
        "                    pe[pos, 2*i] = torch.sin(torch.tensor(\n",
        "                        (col / (n**(2*i / d_model))) +\n",
        "                        (row / (n**(2*i / d_model)))))\n",
        "                    pe[pos, 2*i+1] = torch.cos(torch.tensor(\n",
        "                        (col / (n**(2*i / d_model))) +\n",
        "                        (row / (n**(2*i / d_model)))))\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, emb_dim = x.size()\n",
        "\n",
        "        return x + self.pe[:seq_len, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aIau4RQLAXvd"
      },
      "outputs": [],
      "source": [
        "class WeightedPositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, horizontal_bias=.5, num_rows=4, max_len=5000, n=10000):\n",
        "        super(WeightedPositionalEncoding, self).__init__()\n",
        "\n",
        "        # Assuming max_len is the maximum number of columns\n",
        "        self.num_rows = num_rows\n",
        "        self.d_model = d_model\n",
        "        self.horizontal_bias = horizontal_bias\n",
        "\n",
        "        # Precompute positional encodings for rows and columns\n",
        "        # Rows: fixed at 4\n",
        "        vertical_pe = torch.zeros(num_rows, d_model)\n",
        "        horizontal_pe = torch.zeros(max_len, d_model)\n",
        "\n",
        "        for i in range(num_rows):\n",
        "            for j in range(d_model // 2):\n",
        "                vertical_pe[i, 2*j] = torch.sin(torch.tensor(i / (n**(2*j / d_model))))\n",
        "                vertical_pe[i, 2*j+1] = torch.cos(torch.tensor(i / (n**((2*j+1) / d_model))))\n",
        "\n",
        "        for pos in range(max_len):\n",
        "            for i in range(d_model // 2):\n",
        "                horizontal_pe[pos, 2*i] = torch.sin(torch.tensor(pos / (n**(2*i / d_model))))\n",
        "                horizontal_pe[pos, 2*i+1] = torch.cos(torch.tensor(pos / (n**((2*i+1) / d_model))))\n",
        "\n",
        "        self.register_buffer('vertical_pe', vertical_pe)\n",
        "        self.register_buffer('horizontal_pe', horizontal_pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [batch_size, num_embeddings, d_model]\n",
        "        num_columns = x.size(1) // self.num_rows\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Construct full positional encoding for each element in the grid\n",
        "        full_pe = torch.zeros(batch_size, self.num_rows * num_columns, self.d_model, device=x.device)\n",
        "\n",
        "        for row in range(self.num_rows):\n",
        "            for col in range(num_columns):\n",
        "                full_pe[:, row * num_columns + col, :] = ((1 - self.horizontal_bias) * self.vertical_pe[row]) + (self.horizontal_bias * self.horizontal_pe[col])\n",
        "\n",
        "        return x + full_pe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8koOf5n1Vn1U"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.num_heads = num_heads\n",
        "        self.linear_q = nn.Linear(d_model, d_model)\n",
        "        self.linear_k = nn.Linear(d_model, d_model)\n",
        "        self.linear_v = nn.Linear(d_model, d_model)\n",
        "        self.linear_out = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def attention(self, query, key, value, mask=None):\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        # if mask is not None:\n",
        "        #     scores = scores.masked_fill(mask == 1, -1e9)\n",
        "        if mask is not None:\n",
        "            scores += (mask * -1e9)\n",
        "        attention_weights = torch.nn.functional.softmax(scores, dim=-1)\n",
        "        output = torch.matmul(attention_weights, value)\n",
        "        return output, attention_weights\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        # Linear projections\n",
        "        query = self.linear_q(query)\n",
        "        key = self.linear_k(key)\n",
        "        value = self.linear_v(value)\n",
        "\n",
        "        # Splitting heads\n",
        "        query = query.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        key = key.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        value = value.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Attention\n",
        "        output, attention_weights = self.attention(query, key, value, mask=mask)\n",
        "\n",
        "        # Concatenation of heads\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
        "\n",
        "        # Linear projection\n",
        "        output = self.linear_out(output)\n",
        "\n",
        "        # print(output.shape)\n",
        "        return output, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vxQB2GP4Lcp0"
      },
      "outputs": [],
      "source": [
        "class GroupedMultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, groups):\n",
        "        super(GroupedMultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "        assert num_heads % groups == 0, \"num_heads must be divisible by groups\"\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.groups = groups\n",
        "        self.group_heads = num_heads // groups\n",
        "        self.d_k = d_model // self.group_heads\n",
        "\n",
        "        self.linear_q = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(groups)])\n",
        "        self.linear_k = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(groups)])\n",
        "        self.linear_v = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(groups)])\n",
        "        self.linear_out = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(groups)])\n",
        "\n",
        "    def attention(self, query, key, value, mask=None):\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        print(scores.shape)\n",
        "        print(mask.shape)\n",
        "        if mask is not None:\n",
        "            scores += (mask * -1e9)\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        output = torch.matmul(attention_weights, value)\n",
        "        return output, attention_weights\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        print(\"ATTENTION\")\n",
        "        batch_size = query.size(0)\n",
        "        outputs = []\n",
        "        attention_weights_list = []\n",
        "\n",
        "        for i in range(self.groups):\n",
        "            # Linear projections for each group\n",
        "            q_proj = self.linear_q[i](query)\n",
        "            k_proj = self.linear_k[i](key)\n",
        "            v_proj = self.linear_v[i](value)\n",
        "\n",
        "            # Splitting heads for each group\n",
        "            q_proj = q_proj.view(batch_size, -1, self.group_heads, self.d_k).transpose(1, 2)\n",
        "            k_proj = k_proj.view(batch_size, -1, self.group_heads, self.d_k).transpose(1, 2)\n",
        "            v_proj = v_proj.view(batch_size, -1, self.group_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "            # print(q_proj.shape)\n",
        "            # start_index = i * (query.size(1) // self.groups)\n",
        "            # end_index = (i + 1) * (query.size(1) // self.groups)\n",
        "            # q_proj = q_proj[:, :, start_index:end_index, :]\n",
        "            q_proj = q_proj[:, :, i::4, :]\n",
        "\n",
        "            # Attention for each group\n",
        "            group_output, group_attention_weights = self.attention(q_proj, k_proj, v_proj, mask=mask)\n",
        "\n",
        "\n",
        "            # Concatenation of heads for each group\n",
        "            group_output = group_output.transpose(1, 2).contiguous().view(batch_size, -1, self.group_heads * self.d_k)\n",
        "            # group_output = group_output.transpose(1, 2).contiguous().view(batch_size, self.group_heads, -1, self.d_k)\n",
        "\n",
        "            # Linear projection for each group\n",
        "            group_output = self.linear_out[i](group_output)\n",
        "\n",
        "            outputs.append(group_output)\n",
        "            attention_weights_list.append(group_attention_weights)\n",
        "\n",
        "        return outputs, attention_weights_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0G_NG2bNVpua"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedforward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionwiseFeedforward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.relu(self.linear1(x))\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xjLepzEWVq5y"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = PositionwiseFeedforward(d_model, d_ff)\n",
        "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
        "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # Multi-head self-attention\n",
        "        attn_output, _ = self.self_attn(x, x, x, mask=mask)\n",
        "        # Add & Norm\n",
        "        x = self.layer_norm1(x + self.dropout(attn_output))\n",
        "        # Position-wise feedforward\n",
        "        ffn_output = self.ffn(x)\n",
        "        # Add & Norm\n",
        "        x = self.layer_norm2(x + self.dropout(ffn_output))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "jebf0um3Vr_S"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, d_model, num_layers, num_heads, d_ff, dropout=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.layer_norm(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "3zWuX5Nqo5HH"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.masked_attn_head = MultiHeadAttention(d_model, num_heads)\n",
        "        self.enc_dec_attn_head = MultiHeadAttention(d_model, num_heads)\n",
        "        self.positionwise_ffn = PositionwiseFeedforward(d_model, d_ff)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        # Masked multi-head attention\n",
        "        attn_masked, _ = self.masked_attn_head(x, x, x, mask=tgt_mask)\n",
        "        x = x + self.dropout(attn_masked)\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        # Multi-head attention over encoder's output\n",
        "        attn_enc_dec, _ = self.enc_dec_attn_head(x, enc_output, enc_output, mask=src_mask)\n",
        "        x = x + self.dropout(attn_enc_dec)\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        # Positionwise feedforward\n",
        "        x = x + self.dropout(self.positionwise_ffn(x))\n",
        "        x = self.norm3(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "195837xtMT_H"
      },
      "outputs": [],
      "source": [
        "class GroupedDecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, groups, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        # Grouped multi-head attention for the final layer\n",
        "        self.grouped_attn = GroupedMultiHeadAttention(d_model, num_heads, groups)\n",
        "        self.grouped_norm = nn.ModuleList([nn.LayerNorm(d_model) for _ in range(groups)])\n",
        "        self.grouped_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.positionwise_ffn = nn.ModuleList([PositionwiseFeedforward(d_model, d_ff) for _ in range(groups)])\n",
        "        self.norm2 = nn.ModuleList([nn.LayerNorm(d_model) for _ in range(groups)])\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        # Standard self-attention and add&norm\n",
        "        x = self.norm1(x + self.dropout1(self.self_attn(x, x, x, tgt_mask)[0]))\n",
        "\n",
        "        # Grouped attention; splitting x for each group\n",
        "        grouped_outputs = self.grouped_attn(x, memory, memory, src_mask)[0]\n",
        "\n",
        "        # Processing each group's output independently\n",
        "        final_outputs = []\n",
        "        for i, group_output in enumerate(grouped_outputs):\n",
        "            group_output = self.grouped_norm[i](group_output + self.grouped_dropout(group_output))\n",
        "            group_output = self.norm2[i](group_output + self.dropout2(self.positionwise_ffn[i](group_output)))\n",
        "            final_outputs.append(group_output)\n",
        "\n",
        "        return final_outputs  # Return the list of processed group outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "2LPwrrZwo81J"
      },
      "outputs": [],
      "source": [
        "class OldDecoder(nn.Module):\n",
        "    def __init__(self, d_model, num_layers, num_heads, d_ff, dropout=0.1):\n",
        "        super(OldDecoder, self).__init__()\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, enc_output, src_mask, tgt_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, d_model, num_layers, num_heads, d_ff, groups, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers-1)])\n",
        "        self.final_layer = GroupedDecoderLayer(d_model, num_heads, d_ff, groups, dropout)\n",
        "\n",
        "    def forward(self, tgt, memory, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            tgt = layer(tgt, memory, src_mask, tgt_mask)\n",
        "        return self.final_layer(tgt, memory, src_mask, tgt_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "sZeod3hPVsH6"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, groups, dropout=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding2D(d_model)\n",
        "        self.encoder = Encoder(d_model, num_layers, num_heads, d_ff, dropout)\n",
        "        self.decoder = Decoder(d_model, num_layers, num_heads, d_ff, groups, dropout)\n",
        "        self.output_layers = nn.ModuleList([nn.Linear(d_model, vocab_size) for _ in range(groups)])\n",
        "\n",
        "    def create_padding_mask(self, inputs):\n",
        "        # Create padding mask for inputs\n",
        "        mask = torch.zeros(inputs.shape[0], inputs.shape[1]).to(device)\n",
        "        mask = mask.masked_fill(inputs == 0, 1)\n",
        "        mask = mask.view(inputs.shape[0], 1, 1, inputs.shape[1])\n",
        "\n",
        "    def create_lookahead_mask(self, inputs):\n",
        "        # Create lookahead mask for inputs\n",
        "        mask = torch.triu(torch.ones((inputs.shape[1], inputs.shape[1])), diagonal=1)\n",
        "        print(inputs.shape)\n",
        "        print(mask.shape)\n",
        "        print(mask)\n",
        "        return mask\n",
        "\n",
        "    def forward(self, src_seq, tgt_seq):\n",
        "        src_mask = self.create_padding_mask(src_seq)\n",
        "        print(src_mask.shape)\n",
        "\n",
        "        # Create padding mask and lookahead mask for decoder inputs\n",
        "        padding_mask_dec = self.create_padding_mask(tgt_seq)\n",
        "        lookahead_mask_dec = self.create_lookahead_mask(tgt_seq).to(device)\n",
        "        # Combine padding mask and lookahead mask for decoder\n",
        "        dec_mask = torch.max(padding_mask_dec, lookahead_mask_dec)\n",
        "\n",
        "        print(\"ENCODING\")\n",
        "        src_emb = self.embedding(src_seq)\n",
        "        src_emb = self.positional_encoding(src_emb)\n",
        "        enc_output = self.encoder(src_emb, src_mask)\n",
        "\n",
        "        print(\"DECODING\")\n",
        "        tgt_emb = self.embedding(tgt_seq)\n",
        "        tgt_emb = self.positional_encoding(tgt_emb)\n",
        "        group_outputs = self.decoder(tgt_emb, enc_output, src_mask, dec_mask)\n",
        "\n",
        "        output = [output_layer(group_output) for output_layer, group_output in zip(self.output_layers, group_outputs)]\n",
        "\n",
        "        print(\"OUTPUT\")\n",
        "        print(output[0].shape)\n",
        "        return output\n",
        "\n",
        "\n",
        "class OldTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, groups, dropout=0.1):\n",
        "        super(OldTransformer, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding2D(d_model)\n",
        "        self.encoder = Encoder(d_model, num_layers, num_heads, d_ff, dropout)\n",
        "        self.decoder = OldDecoder(d_model, num_layers, num_heads, d_ff, dropout)\n",
        "        self.output_linear = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def create_padding_mask(self, inputs):\n",
        "        # Create padding mask for inputs\n",
        "        mask = torch.zeros(inputs.shape[0], inputs.shape[1]).to(device)\n",
        "        mask = mask.masked_fill(inputs == 0, 1)\n",
        "        mask = mask.view(inputs.shape[0], 1, 1, inputs.shape[1])\n",
        "        return mask\n",
        "\n",
        "    def create_lookahead_mask(self, inputs):\n",
        "        # Create lookahead mask for inputs\n",
        "        mask = torch.triu(torch.ones((inputs.shape[1], inputs.shape[1])), diagonal=1)\n",
        "        return mask\n",
        "\n",
        "    def forward(self, src_seq, tgt_seq):\n",
        "        src_mask = self.create_padding_mask(src_seq).to(device)\n",
        "\n",
        "        # Create padding mask and lookahead mask for decoder inputs\n",
        "        padding_mask_dec = self.create_padding_mask(tgt_seq).to(device)\n",
        "        lookahead_mask_dec = self.create_lookahead_mask(tgt_seq).to(device)\n",
        "        # Combine padding mask and lookahead mask for decoder\n",
        "        dec_mask = torch.max(padding_mask_dec, lookahead_mask_dec).to(device)\n",
        "\n",
        "        # print(\"ENCODING\")\n",
        "        src_emb = self.embedding(src_seq)\n",
        "        src_emb = self.positional_encoding(src_emb)\n",
        "        enc_output = self.encoder(src_emb, src_mask)\n",
        "\n",
        "        # print(\"DECODING\")\n",
        "        tgt_emb = self.embedding(tgt_seq)\n",
        "        tgt_emb = self.positional_encoding(tgt_emb)\n",
        "        dec_output = self.decoder(tgt_emb, enc_output, src_mask, dec_mask)\n",
        "\n",
        "        output = self.output_linear(dec_output)\n",
        "\n",
        "        # print(\"OUTPUT\")\n",
        "        # print(output.shape)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "\n",
        "torch.mps.set_per_process_memory_fraction(2.0)\n",
        "\n",
        "PATH = \"1-PositionalEncodings.pt\"\n",
        "\n",
        "# INPUT FORMAT\n",
        "# Step1_Voice1, Step1_Voice2, Step1_Voice3, Step1_Voice4, Step2..., EOS,\n",
        "# TARGET_CLS_TOKEN_ID, Voice1, ... Voice 4, PAD, PAD, ...\n",
        "#Alternative, target chord in final layer of decoder?\n",
        "\n",
        "# Define some hyperparameters\n",
        "d_model = 64\n",
        "num_layers = 6\n",
        "num_heads = 32\n",
        "d_ff = 128\n",
        "dropout = 0.1\n",
        "batch_size = 32\n",
        "voices = 4\n",
        "\n",
        "# train_dataset = TensorDataset(src_data, tgt_data)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Initialize the Transformer model\n",
        "model = OldTransformer(VOCAB_SIZE, d_model, num_layers, num_heads, d_ff, voices, dropout).to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4ccONcYWBHV",
        "outputId": "9e8f79ab-d0d2-4b17-8924-cb09ed2f70b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss: 1.5504\n",
            "tensor([39, 31, 17, 36, 36, 29, 17, 36, 36, 29, 17, 36, 36, 34, 17, 36, 34, 36,\n",
            "        38, 39, 36, 36, 38, 39, 34, 36, 19, 39, 39, 36, 19, 39],\n",
            "       device='mps:0')\n",
            "tensor([36, 29, 17, 44, 36, 29, 17, 44, 38, 29, 17, 44, 38, 29, 17, 43, 39, 22,\n",
            "        19, 43, 39, 22, 19, 43, 39, 22, 20, 43, 39, 22, 20, 41],\n",
            "       device='mps:0')\n",
            "Epoch 2 Loss: 1.4096\n",
            "tensor([33, 22, 22, 33, 29, 26, 19, 38, 29, 26, 19, 38, 31, 26, 19, 38, 31, 26,\n",
            "        19, 38, 31, 24, 17, 38, 31, 26, 39, 39, 31, 24, 17, 39],\n",
            "       device='mps:0')\n",
            "tensor([29, 26, 11, 38, 29, 26, 11, 38, 31, 26, 11, 38, 31, 26, 11, 39, 31, 24,\n",
            "        12, 39, 31, 24, 12, 39, 31, 24, 12, 39, 31, 24, 12, 34],\n",
            "       device='mps:0')\n",
            "Epoch 3 Loss: 1.3111\n",
            "tensor([32, 29, 27, 20, 32, 27, 24, 39, 32, 27, 24, 39, 32, 27, 24, 39, 32, 27,\n",
            "        41, 13, 32, 27, 41, 41, 34, 25, 24, 13, 34, 27, 24, 13],\n",
            "       device='mps:0')\n",
            "tensor([32, 27, 24, 39, 32, 27, 24, 39, 32, 27, 24, 39, 32, 27, 24, 41, 32, 25,\n",
            "        25, 41, 32, 25, 25, 42, 32, 27, 25, 42, 32, 27, 25, 44],\n",
            "       device='mps:0')\n",
            "Epoch 4 Loss: 1.2193\n",
            "tensor([35, 31, 24, 38, 35, 31, 19, 38, 35, 31, 19, 38, 35, 31, 19, 38, 31, 19,\n",
            "        12, 36, 31, 28, 12, 36, 31, 28, 12, 36, 31, 28, 12, 36],\n",
            "       device='mps:0')\n",
            "tensor([35, 31, 19, 38, 35, 31, 19, 38, 35, 29, 19, 38, 35, 29, 19, 36, 31, 28,\n",
            "        12, 36, 31, 28, 12, 36, 31, 28, 12, 36, 31, 28, 12, 40],\n",
            "       device='mps:0')\n",
            "Epoch 5 Loss: 1.1226\n",
            "tensor([33, 28, 24, 36, 31, 28, 12, 36, 31, 28, 24, 36, 31, 28, 14, 36, 31, 12,\n",
            "        28, 36, 31, 16, 28, 36, 31, 21, 28, 36, 31, 12, 28, 36],\n",
            "       device='mps:0')\n",
            "tensor([31, 28, 12, 36, 31, 28, 12, 36, 31, 29, 14, 36, 31, 29, 14, 36, 24, 31,\n",
            "        16, 36, 24, 31, 16, 36, 24, 31, 16, 36, 24, 31, 16,  1],\n",
            "       device='mps:0')\n"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "\n",
        "checkpoint = torch.load(PATH)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for src_seq, tgt_seq in train_loader:\n",
        "        src_seq = src_seq.to(device)\n",
        "        tgt_seq = tgt_seq.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        tgt_target = torch.cat((tgt_seq[:, 1:], torch.full_like(tgt_seq[:, :1], PAD_TOKEN_ID)), dim=1)\n",
        "\n",
        "        output = model(src_seq, tgt_seq)\n",
        "\n",
        "        # output_indexes = torch.argmax(output, dim=-1)\n",
        "        # print(output_indexes[0])\n",
        "        # print(tgt_seq[0])\n",
        "        # print(tgt_target[0])\n",
        "\n",
        "        loss = criterion(output.view(-1, VOCAB_SIZE), tgt_target.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        gc.collect()\n",
        "        torch.mps.empty_cache() \n",
        "\n",
        "    checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }\n",
        "    torch.save(checkpoint, PATH)\n",
        "    \n",
        "    if (epoch+1) % 1 == 0:\n",
        "        print('Epoch {} Loss: {:.4f}'.format(epoch+1, total_loss / len(train_loader)))\n",
        "        \n",
        "        # Print indexes of max values in the model output\n",
        "        # print(\"Model output (indexes of max values):\", output_indexes.view(-1)[:10])  # Example: print first 10 values\n",
        "        output_indexes = torch.argmax(output, dim=-1)\n",
        "        print(output_indexes[0][:32])\n",
        "        # print(tgt_seq[0])\n",
        "        print(tgt_target[0][:32])\n",
        "\n",
        "        # print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "# # Evaluation\n",
        "# model.eval()\n",
        "# with torch.no_grad():\n",
        "#     test_output = model(test_src_data, test_tgt_data, test_src_mask)\n",
        "#     test_loss = criterion(test_output.view(-1, tgt_vocab_size), test_tgt_data.view(-1))\n",
        "#     print(f\"Test Loss: {test_loss.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnOIfxVdF9ze",
        "outputId": "78a952bd-b8a9-4593-b89c-cff60ce54bc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating Chorale\n",
            "Generating Score\n",
            "Exporting Score\n"
          ]
        }
      ],
      "source": [
        "# hand-evaluation\n",
        "\n",
        "# temp=.5 (.4-.7) best\n",
        "def generate_next_tokens(model, src_seq, tgt_prefix, max_len=320, temperature=.6):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        src_seq = src_seq.unsqueeze(0).to(device)\n",
        "        tgt_seq = tgt_prefix.unsqueeze(0).to(device)\n",
        "        for i in range(max_len):\n",
        "            output = model(src_seq, tgt_seq)\n",
        "            output_last_token = output[:, -1, :]  # Take the last token of the output\n",
        "            # next_token = torch.argmax(output_last_token, dim=-1).unsqueeze(1)\n",
        "            probabilities = torch.nn.functional.softmax(output_last_token / temperature, dim=-1)\n",
        "            next_token = torch.multinomial(probabilities, 1)  # Sample the next token\n",
        "            tgt_seq = torch.cat([tgt_seq, next_token], dim=-1)\n",
        "            if next_token == EOS_TOKEN_ID:  # Add a condition to stop generation\n",
        "                break\n",
        "    return tgt_seq.squeeze(0)\n",
        "\n",
        "def to_score_form(seq, name=\"Unknown\"):\n",
        "    def id_to_note(id):\n",
        "        if id == NO_NOTE_TOKEN_ID or id == PAD_TOKEN_ID:\n",
        "            return 0\n",
        "        return id + NOTE_OFFSET\n",
        "\n",
        "    seq = seq.tolist()\n",
        "    score = [[] for _ in range(4)]\n",
        "    for i in range(len(seq)):\n",
        "        score[i % 4].append(id_to_note(seq[i]))\n",
        "    return [name, score]\n",
        "\n",
        "gc.collect()\n",
        "torch.mps.empty_cache() \n",
        "\n",
        "test_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "for src_seq, tgt_seq in test_loader:\n",
        "    print(\"Generating Chorale\")\n",
        "    res = generate_next_tokens(model, src_seq[0], tgt_seq[0][:4])\n",
        "    print(\"Generating Scores\")\n",
        "    score = to_score_form(res, \"Generated Chorale\")\n",
        "    src_score = to_score_form(src_seq[0], \"Source Chorale\")\n",
        "    tgt_score = to_score_form(tgt_seq[0], \"Target Chorale\")\n",
        "    print(\"Exporting Scores\")\n",
        "    visualize_score(src_score, \"2-LossFunction-Src\")\n",
        "    visualize_score(tgt_score, \"2-LossFunction-Tgt\")\n",
        "    visualize_score(score, \"2-LossFunction-Gen\")\n",
        "\n",
        "    gc.collect()\n",
        "    torch.mps.empty_cache() \n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "del model\n",
        "torch.mps.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
