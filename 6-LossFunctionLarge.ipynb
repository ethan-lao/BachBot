{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVtNSBCCVHDX"
      },
      "source": [
        "# Dataset and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myWtI3bwxc6c",
        "outputId": "5b620a18-4e56-470c-b703-2edeea9f40d3"
      },
      "outputs": [],
      "source": [
        "# get chorale dataset\n",
        "# https://github.com/ageron/handson-ml2/blob/master/datasets/jsb_chorales/README.md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "device='mps' if torch.backends.mps.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2i9LrwDc0Btv",
        "outputId": "c7ebe1fe-4473-4d30-9bfa-ad115f465fde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[36, 81]\n",
            "[[60, 81], [52, 74], [46, 69], [36, 66]]\n"
          ]
        }
      ],
      "source": [
        "# preprocess the chorales\n",
        "import os\n",
        "import csv\n",
        "\n",
        "note_range = [88, 0]\n",
        "voice_ranges = [[88, 0] for _ in range(4)]\n",
        "\n",
        "def parse_score_dir(folder_path):\n",
        "    scores = []\n",
        "\n",
        "    # Loop through all files in the directory\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.csv'):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            #print(f\"Contents of {filename}:\")\n",
        "\n",
        "            # Open the CSV file\n",
        "            with open(file_path, 'r') as csv_file:\n",
        "                csv_reader = csv.reader(csv_file)\n",
        "                next(csv_reader) # skip header\n",
        "\n",
        "                voices = [[] for _ in range(4)]\n",
        "                for row in csv_reader:\n",
        "                    for i in range(4):\n",
        "                        note = int(row[i])\n",
        "\n",
        "                        voices[i].append(note)\n",
        "\n",
        "                        if note != 0:\n",
        "                            note_range[0] = min(note_range[0], note)\n",
        "                            note_range[1] = max(note_range[1], note)\n",
        "\n",
        "                            voice_ranges[i][0] = min(voice_ranges[i][0], note)\n",
        "                            voice_ranges[i][1] = max(voice_ranges[i][1], note)\n",
        "\n",
        "                scores.append([filename, voices])\n",
        "\n",
        "    return scores\n",
        "\n",
        "test = parse_score_dir(\"test\")\n",
        "train = parse_score_dir(\"train\")\n",
        "valid = parse_score_dir(\"valid\")\n",
        "\n",
        "print(note_range)\n",
        "print(voice_ranges)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FIj_FBJEH73e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num in train: 7652\n",
            "Num in valid: 1216\n",
            "Num in test: 954\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "from secrets import randbelow\n",
        "import torch\n",
        "\n",
        "def rand_num(min, max_excl):\n",
        "    return randbelow(int(max_excl - min)) + min\n",
        "\n",
        "def split_scores(scores, repeats=20):\n",
        "    res = []\n",
        "    for score in scores:\n",
        "        filename, voices = score\n",
        "        for r in range(repeats):\n",
        "            num_beats = len(voices[0]) / 4\n",
        "            src_seq_length = rand_num(16, 21)\n",
        "\n",
        "            # pick random index for training seq\n",
        "            i = randbelow(int(num_beats - src_seq_length + 1))\n",
        "            src_voices = [v[i*4:(i+src_seq_length)*4] for v in voices]\n",
        "\n",
        "            # pick index for tgt\n",
        "            tgt_voices = None\n",
        "            if i > (num_beats - (i + src_seq_length)):\n",
        "                # tgt at beginning\n",
        "                if i <= 17:\n",
        "                    continue\n",
        "                tgt_seq_length = min(20, rand_num(17, i))\n",
        "                tgt_i = randbelow(i - tgt_seq_length + 1)\n",
        "                tgt_voices = [v[tgt_i*4:(tgt_i+tgt_seq_length)*4] for v in voices]\n",
        "            else:\n",
        "                # tgt at end\n",
        "                if (num_beats - (i + src_seq_length)) <= 17:\n",
        "                    continue\n",
        "                tgt_seq_length = min(20, rand_num(17, (num_beats - (i + src_seq_length))))\n",
        "                tgt_i = rand_num(i + src_seq_length + 1, num_beats - tgt_seq_length + 1)\n",
        "                tgt_voices = [v[tgt_i*4:(tgt_i+tgt_seq_length)*4] for v in voices]\n",
        "\n",
        "            res.append([filename, src_voices, tgt_voices])\n",
        "\n",
        "    return res\n",
        "\n",
        "split_train = split_scores(train, 40)\n",
        "split_test = split_scores(test, 15)\n",
        "split_valid = split_scores(valid)\n",
        "\n",
        "# max_src = 0\n",
        "# max_tgt = 0\n",
        "# for s in split_train:\n",
        "#     max_src = max(len(s[1][0]), max_src)\n",
        "#     max_tgt = max(len(s[2][0]), max_tgt)\n",
        "\n",
        "# print(max_src)\n",
        "# print(max_tgt)\n",
        "\n",
        "print(f\"Num in train: {len(split_train)}\")\n",
        "print(f\"Num in valid: {len(split_valid)}\")\n",
        "print(f\"Num in test: {len(split_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7Cfx3in5SRer"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "\n",
        "PAD_TOKEN_ID = 0\n",
        "EOS_TOKEN_ID = 1\n",
        "NO_NOTE_TOKEN_ID = 2\n",
        "NOTE_OFFSET = note_range[0] - 3\n",
        "VOCAB_SIZE = 3 + note_range[1] - note_range[0] + 1\n",
        "\n",
        "def to_dataset(split_scores, max_src_seq_len=324, max_tgt_seq_len=324):\n",
        "    src_data = []\n",
        "    tgt_data = []\n",
        "    for score in split_scores:\n",
        "        filename, src_voices, tgt_voices = score\n",
        "\n",
        "        def note_to_id(note):\n",
        "            if note == 0:\n",
        "                return NO_NOTE_TOKEN_ID\n",
        "            return note - NOTE_OFFSET\n",
        "\n",
        "        def voices_to_src_seq(voices):\n",
        "            seq = torch.zeros(max_src_seq_len, dtype=torch.int64)\n",
        "            for i in range(len(voices[0])):\n",
        "                seq[i*4] = note_to_id(voices[0][i])\n",
        "                seq[i*4 + 1] = note_to_id(voices[1][i])\n",
        "                seq[i*4 + 2] = note_to_id(voices[2][i])\n",
        "                seq[i*4 + 3] = note_to_id(voices[3][i])\n",
        "            return seq\n",
        "\n",
        "        def voices_to_tgt_seq(voices):\n",
        "            seq = torch.zeros(max_tgt_seq_len, dtype=torch.int64)\n",
        "            for i in range(len(voices[0])):\n",
        "                seq[i*4] = note_to_id(voices[0][i])\n",
        "                seq[i*4 + 1] = note_to_id(voices[1][i])\n",
        "                seq[i*4 + 2] = note_to_id(voices[2][i])\n",
        "                seq[i*4 + 3] = note_to_id(voices[3][i])\n",
        "            seq[(i+1)*4] = EOS_TOKEN_ID\n",
        "            seq[(i+1)*4 + 1] = EOS_TOKEN_ID\n",
        "            seq[(i+1)*4 + 2] = EOS_TOKEN_ID\n",
        "            seq[(i+1)*4 + 3] = EOS_TOKEN_ID\n",
        "            return seq\n",
        "\n",
        "        src_data.append(voices_to_src_seq(src_voices))\n",
        "        tgt_data.append(voices_to_tgt_seq(tgt_voices))\n",
        "\n",
        "    dataset = TensorDataset(torch.stack(src_data).to(device), torch.stack(tgt_data).to(device))\n",
        "    return dataset\n",
        "\n",
        "train_dataset = to_dataset(split_train)\n",
        "test_dataset = to_dataset(split_test)\n",
        "# valid_dataset = to_dataset(split_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES2WYCMlVLNw"
      },
      "source": [
        "# Score Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5Vk_jCARyVDo"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from pathlib import Path\n",
        "import musicscore\n",
        "from musicscore import *\n",
        "import uuid\n",
        "\n",
        "gen_id = lambda: f\"a{str(uuid.uuid4()).replace('-', '')}\"\n",
        "\n",
        "def visualize_score(score, name):\n",
        "    title, content = score\n",
        "    newScore = Score(title=title)\n",
        "\n",
        "    soprano = newScore.add_child(Part(gen_id(), name='Soprano'))\n",
        "    alto = newScore.add_child(Part(gen_id(), name='Alto'))\n",
        "    tenor = newScore.add_child(Part(gen_id(), name='Tenor'))\n",
        "    bass = newScore.add_child(Part(gen_id(), name='Bass'))\n",
        "\n",
        "    parts = [soprano, alto, tenor, bass]\n",
        "    clefs = [TrebleClef(), TrebleClef(), BassClef(), BassClef()]\n",
        "\n",
        "    for part_index in range(4):\n",
        "        part_content = content[part_index]\n",
        "        part = parts[part_index]\n",
        "        clef = clefs[part_index]\n",
        "\n",
        "        if (len(part_content) % 16 != 0 and False):\n",
        "            # add pickup measure\n",
        "            pickuptime = musicscore.time.Time()\n",
        "            pickuptime.actual_signatures = [1, 4]\n",
        "\n",
        "            measure = part.add_child(Measure(number=1, time=pickuptime))\n",
        "            staff = measure.add_child(Staff(clef=clef))\n",
        "\n",
        "            normaltime = musicscore.time.Time()\n",
        "            measure = part.add_child(Measure(number=2, time=normaltime))\n",
        "        else:\n",
        "            measure = part.add_child(Measure(number=1))\n",
        "            staff = measure.add_child(Staff(clef=clef))\n",
        "\n",
        "        note_index = 0\n",
        "        while note_index < len(part_content):\n",
        "            note = part_content[note_index]\n",
        "            duration = .25\n",
        "            while (note_index != len(part_content) - 1 and (note_index + 1) % 4 != 0 and note == part_content[note_index + 1]):\n",
        "                duration += .25\n",
        "                note_index += 1\n",
        "\n",
        "            accidental = musicscore.accidental.Accidental(\"sharp\")\n",
        "\n",
        "            #check if next note is descending\n",
        "            check_index = note_index + 1\n",
        "            while True:\n",
        "                if check_index == len(part_content):\n",
        "                    break;\n",
        "\n",
        "                check_note = part_content[check_index]\n",
        "                if (check_note != note):\n",
        "                    if check_note < note and note - check_note < 3:\n",
        "                        accidental.mode = \"flat\"\n",
        "                    break;\n",
        "\n",
        "                check_index += 1\n",
        "\n",
        "\n",
        "            midi = musicscore.midi.Midi(note, accidental)\n",
        "            chord = Chord(midi, duration)\n",
        "            part.add_chord(chord)\n",
        "            note_index += 1\n",
        "\n",
        "    xml_path = Path(name).with_suffix('.xml')\n",
        "    newScore.export_xml(xml_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfMxIh27VeF3"
      },
      "source": [
        "# Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZFhQI7hDVmaW"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000, n=10000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        # pe = torch.zeros(max_len, d_model)\n",
        "        # position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        # div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        # pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        # pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        # pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        # self.register_buffer('pe', pe)\n",
        "\n",
        "        # Generate position encoding based on the given sequence length, hidden size, and frequency (n)\n",
        "\n",
        "        horizontal_pe = torch.zeros(max_len, d_model)\n",
        "\n",
        "        for pos in range(max_len):\n",
        "            for i in range(d_model // 2):\n",
        "                horizontal_pe[pos, 2*i] = torch.sin(torch.tensor(pos / (n**(2*i / d_model))))\n",
        "                horizontal_pe[pos, 2*i+1] = torch.cos(torch.tensor(pos / (n**(2*i / d_model))))\n",
        "        self.register_buffer('horizontal_pe', horizontal_pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(x.shape)\n",
        "        # print(self.horizontal_pe[:x.size(1), :].shape)\n",
        "        return x + self.horizontal_pe[:x.size(1), :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gAC63HS_CUxu"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding2D(nn.Module):\n",
        "    def __init__(self, d_model, max_len=320, n=10000):\n",
        "        super(PositionalEncoding2D, self).__init__()\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "        self.n = n\n",
        "\n",
        "        # Initialize positional encodings\n",
        "        pe = torch.zeros(max_len * 4, d_model)\n",
        "        for col in range(max_len):\n",
        "            for row in range(4):\n",
        "                pos = (row * 4) + col\n",
        "                for i in range(d_model // 2):\n",
        "                    pe[pos, 2*i] = torch.sin(torch.tensor(\n",
        "                        (col / (n**(2*i / d_model))) +\n",
        "                        (row / (n**(2*i / d_model)))))\n",
        "                    pe[pos, 2*i+1] = torch.cos(torch.tensor(\n",
        "                        (col / (n**(2*i / d_model))) +\n",
        "                        (row / (n**(2*i / d_model)))))\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, emb_dim = x.size()\n",
        "\n",
        "        return x + self.pe[:seq_len, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8koOf5n1Vn1U"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.num_heads = num_heads\n",
        "        self.linear_q = nn.Linear(d_model, d_model)\n",
        "        self.linear_k = nn.Linear(d_model, d_model)\n",
        "        self.linear_v = nn.Linear(d_model, d_model)\n",
        "        self.linear_out = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def attention(self, query, key, value, mask=None):\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        # if mask is not None:\n",
        "        #     scores = scores.masked_fill(mask == 1, -1e9)\n",
        "        if mask is not None:\n",
        "            scores += (mask * -1e9)\n",
        "        attention_weights = torch.nn.functional.softmax(scores, dim=-1)\n",
        "        output = torch.matmul(attention_weights, value)\n",
        "        return output, attention_weights\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        # Linear projections\n",
        "        query = self.linear_q(query)\n",
        "        key = self.linear_k(key)\n",
        "        value = self.linear_v(value)\n",
        "\n",
        "        # Splitting heads\n",
        "        query = query.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        key = key.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        value = value.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Attention\n",
        "        output, attention_weights = self.attention(query, key, value, mask=mask)\n",
        "\n",
        "        # Concatenation of heads\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
        "\n",
        "        # Linear projection\n",
        "        output = self.linear_out(output)\n",
        "\n",
        "        # print(output.shape)\n",
        "        return output, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0G_NG2bNVpua"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedforward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionwiseFeedforward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.relu(self.linear1(x))\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "xjLepzEWVq5y"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = PositionwiseFeedforward(d_model, d_ff)\n",
        "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
        "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # Multi-head self-attention\n",
        "        attn_output, _ = self.self_attn(x, x, x, mask=mask)\n",
        "        # Add & Norm\n",
        "        x = self.layer_norm1(x + self.dropout(attn_output))\n",
        "        # Position-wise feedforward\n",
        "        ffn_output = self.ffn(x)\n",
        "        # Add & Norm\n",
        "        x = self.layer_norm2(x + self.dropout(ffn_output))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jebf0um3Vr_S"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, d_model, num_layers, num_heads, d_ff, dropout=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.layer_norm(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3zWuX5Nqo5HH"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.masked_attn_head = MultiHeadAttention(d_model, num_heads)\n",
        "        self.enc_dec_attn_head = MultiHeadAttention(d_model, num_heads)\n",
        "        self.positionwise_ffn = PositionwiseFeedforward(d_model, d_ff)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        # Masked multi-head attention\n",
        "        attn_masked, _ = self.masked_attn_head(x, x, x, mask=tgt_mask)\n",
        "        x = x + self.dropout(attn_masked)\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        # Multi-head attention over encoder's output\n",
        "        attn_enc_dec, _ = self.enc_dec_attn_head(x, enc_output, enc_output, mask=src_mask)\n",
        "        x = x + self.dropout(attn_enc_dec)\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        # Positionwise feedforward\n",
        "        x = x + self.dropout(self.positionwise_ffn(x))\n",
        "        x = self.norm3(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2LPwrrZwo81J"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, d_model, num_layers, num_heads, d_ff, dropout=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, enc_output, src_mask, tgt_mask)\n",
        "        return self.norm(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "sZeod3hPVsH6"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, groups, dropout=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding2D(d_model)\n",
        "        self.encoder = Encoder(d_model, num_layers, num_heads, d_ff, dropout)\n",
        "        self.decoder = Decoder(d_model, num_layers, num_heads, d_ff, dropout)\n",
        "        self.output_linear = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def create_padding_mask(self, inputs):\n",
        "        # Create padding mask for inputs\n",
        "        mask = torch.zeros(inputs.shape[0], inputs.shape[1]).to(device)\n",
        "        mask = mask.masked_fill(inputs == 0, 1)\n",
        "        mask = mask.view(inputs.shape[0], 1, 1, inputs.shape[1])\n",
        "        return mask\n",
        "\n",
        "    def create_lookahead_mask(self, inputs):\n",
        "        # Create lookahead mask for inputs\n",
        "        mask = torch.triu(torch.ones((inputs.shape[1], inputs.shape[1])), diagonal=1)\n",
        "        return mask\n",
        "\n",
        "    def forward(self, src_seq, tgt_seq):\n",
        "        src_mask = self.create_padding_mask(src_seq).to(device)\n",
        "\n",
        "        # Create padding mask and lookahead mask for decoder inputs\n",
        "        padding_mask_dec = self.create_padding_mask(tgt_seq).to(device)\n",
        "        lookahead_mask_dec = self.create_lookahead_mask(tgt_seq).to(device)\n",
        "        # Combine padding mask and lookahead mask for decoder\n",
        "        dec_mask = torch.max(padding_mask_dec, lookahead_mask_dec).to(device)\n",
        "\n",
        "        # print(\"ENCODING\")\n",
        "        src_emb = self.embedding(src_seq)\n",
        "        src_emb = self.positional_encoding(src_emb)\n",
        "        enc_output = self.encoder(src_emb, src_mask)\n",
        "\n",
        "        # print(\"DECODING\")\n",
        "        tgt_emb = self.embedding(tgt_seq)\n",
        "        tgt_emb = self.positional_encoding(tgt_emb)\n",
        "        dec_output = self.decoder(tgt_emb, enc_output, src_mask, dec_mask)\n",
        "\n",
        "        output = self.output_linear(dec_output)\n",
        "\n",
        "        # print(\"OUTPUT\")\n",
        "        # print(output.shape)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "\n",
        "class CustomLoss(nn.Module):\n",
        "    def __init__(self, voice_ranges, downbeat_weight=2, note_change_weight=2, voice_range_weight=4):\n",
        "        super(CustomLoss, self).__init__()\n",
        "\n",
        "        self.voice_range_weight = voice_range_weight\n",
        "        self.downbeat_weight = downbeat_weight\n",
        "        self.note_change_weight = note_change_weight\n",
        "\n",
        "        # [[27, 48], [19, 41], [13, 36], [3, 33]]\n",
        "        self.voice_ranges = copy.deepcopy(voice_ranges)\n",
        "        for r in range(len(self.voice_ranges)):\n",
        "            for i in range(len(self.voice_ranges[r])):\n",
        "                self.voice_ranges[r][i] -= NOTE_OFFSET\n",
        "\n",
        "        self.cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    def cross_entropy(self, predictions, tgt_seq, targets):\n",
        "        # Cross Entropy Loss\n",
        "        max_logits = predictions.max(dim=2, keepdim=True)[0]  # For numerical stability\n",
        "        exp_logits = torch.exp(predictions - max_logits)\n",
        "        softmax_probs = exp_logits / exp_logits.sum(dim=2, keepdim=True)\n",
        "\n",
        "        gathered_probs = softmax_probs.gather(dim=2, index=targets.unsqueeze(2)).squeeze(2)\n",
        "        nll = -torch.log(gathered_probs + 1e-9) \n",
        "\n",
        "        # downbeat bias\n",
        "        seq_len = nll.size(1)\n",
        "        down_beat_mask = torch.zeros(seq_len // 4, 4, dtype=torch.bool)\n",
        "        down_beat_mask[::4, :] = True\n",
        "        down_beat_mask = down_beat_mask.view(-1)\n",
        "        down_beat_mask = torch.roll(down_beat_mask, shifts=-1)\n",
        "        nll[:, down_beat_mask] *= self.downbeat_weight\n",
        "\n",
        "        # note-change bias\n",
        "        note_change_mask = torch.zeros_like(nll, dtype=torch.bool)\n",
        "        note_change_mask[:, 4:] = tgt_seq[:, 4:] != tgt_seq[:, :-4]\n",
        "        note_change_mask_clone = note_change_mask.clone()\n",
        "        note_change_mask[:, :-1] = note_change_mask_clone[:, 1:]\n",
        "        note_change_mask[:, -1] = False\n",
        "        nll[note_change_mask] *= self.note_change_weight\n",
        "\n",
        "        total_loss = nll.mean()\n",
        "\n",
        "        return total_loss\n",
        "    \n",
        "    def voice_range_loss(self, predictions, targets):\n",
        "        pred_tokens = predictions.argmax(dim=-1)\n",
        "        not_note_mask = (pred_tokens <= 2)\n",
        "        divisor = 1\n",
        "\n",
        "        # soprano voice, notes too low\n",
        "        sop_mask = torch.zeros_like(pred_tokens, dtype=torch.bool)\n",
        "        sop_mask[:, 3::4] = True\n",
        "        penalty_below_0 = (self.voice_ranges[0][0] - pred_tokens) * (pred_tokens < self.voice_ranges[0][0]) / divisor\n",
        "        penalty_below_0 = penalty_below_0.masked_fill(not_note_mask, 0)\n",
        "        penalty_below_0 = penalty_below_0.masked_fill(~sop_mask, 0)\n",
        "\n",
        "        # # soprano voice, notes too high\n",
        "        # penalty_above_0 = (pred_tokens - self.voice_ranges[0][1]) * (pred_tokens > self.voice_ranges[0][1]) / divisor\n",
        "        # penalty_above_0 = penalty_above_0.masked_fill(not_note_mask, 0)\n",
        "        # penalty_above_0 = penalty_above_0.masked_fill(~sop_mask, 0)\n",
        "\n",
        "\n",
        "        # alto voice, notes too low\n",
        "        alto_mask = torch.zeros_like(pred_tokens, dtype=torch.bool)\n",
        "        alto_mask[:, 0::4] = True\n",
        "        penalty_below_1 = (self.voice_ranges[1][0] - pred_tokens) * (pred_tokens < self.voice_ranges[1][0]) / divisor\n",
        "        penalty_below_1 = penalty_below_1.masked_fill(not_note_mask, 0)\n",
        "        penalty_below_1 = penalty_below_1.masked_fill(~alto_mask, 0)\n",
        "\n",
        "        # alto voice, notes too high\n",
        "        penalty_above_1 = (pred_tokens - self.voice_ranges[1][1]) * (pred_tokens > self.voice_ranges[1][1]) / divisor\n",
        "        penalty_above_1 = penalty_above_1.masked_fill(not_note_mask, 0)\n",
        "        penalty_above_1 = penalty_above_1.masked_fill(~alto_mask, 0)\n",
        "\n",
        "        \n",
        "        # tenor voice, notes too low\n",
        "        tenor_mask = torch.zeros_like(pred_tokens, dtype=torch.bool)\n",
        "        tenor_mask[:, 1::4] = True\n",
        "        penalty_below_2 = (self.voice_ranges[2][0] - pred_tokens) * (pred_tokens < self.voice_ranges[2][0]) / divisor\n",
        "        penalty_below_2 = penalty_below_2.masked_fill(not_note_mask, 0)\n",
        "        penalty_below_2 = penalty_below_2.masked_fill(~tenor_mask, 0)\n",
        "\n",
        "        # tenor voice, notes too high\n",
        "        penalty_above_2 = (pred_tokens - self.voice_ranges[2][1]) * (pred_tokens > self.voice_ranges[2][1]) / divisor\n",
        "        penalty_above_2 = penalty_above_2.masked_fill(not_note_mask, 0)\n",
        "        penalty_above_2 = penalty_above_2.masked_fill(~tenor_mask, 0)\n",
        "\n",
        "\n",
        "        # bass voice, notes too low\n",
        "        bass_mask = torch.zeros_like(pred_tokens, dtype=torch.bool)\n",
        "        bass_mask[:, 2::4] = True\n",
        "        # penalty_below_3 = (self.voice_ranges[3][0] - pred_tokens) * (pred_tokens < self.voice_ranges[3][0]) / divisor\n",
        "        # penalty_below_3 = penalty_below_3.masked_fill(not_note_mask, 0)\n",
        "        # penalty_below_3 = penalty_below_3.masked_fill(~bass_mask, 0)\n",
        "\n",
        "        # bass voice, notes too high\n",
        "        penalty_above_3 = (pred_tokens - self.voice_ranges[3][1]) * (pred_tokens > self.voice_ranges[3][1]) / divisor\n",
        "        penalty_above_3 = penalty_above_3.masked_fill(not_note_mask, 0)\n",
        "        penalty_above_3 = penalty_above_3.masked_fill(~bass_mask, 0)\n",
        "\n",
        "\n",
        "        # total\n",
        "        total_penalty = (penalty_below_0 + penalty_below_1 + penalty_below_2 + penalty_above_1 + penalty_above_2 + penalty_above_3)\n",
        "        mean_penalty = total_penalty.mean()\n",
        "        return mean_penalty\n",
        "\n",
        "    def forward(self, predictions, tgt_seq, targets):\n",
        "        l1 = self.voice_range_weight * self.voice_range_loss(predictions, targets)\n",
        "        l2 = self.cross_entropy(predictions, tgt_seq, targets)\n",
        "        total_loss = l1 + l2\n",
        "        return total_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.mps.set_per_process_memory_fraction(2.0)\n",
        "\n",
        "PATH = \"6-LossFunctionLarge.pt\"\n",
        "\n",
        "# INPUT FORMAT\n",
        "# Step1_Voice1, Step1_Voice2, Step1_Voice3, Step1_Voice4, Step2..., EOS,\n",
        "# TARGET_CLS_TOKEN_ID, Voice1, ... Voice 4, PAD, PAD, ...\n",
        "#Alternative, target chord in final layer of decoder?\n",
        "\n",
        "# Define some hyperparameters\n",
        "d_model = 128\n",
        "num_layers = 5\n",
        "num_heads = 32\n",
        "d_ff = 256\n",
        "dropout = 0.1\n",
        "batch_size = 64\n",
        "voices = 4\n",
        "\n",
        "# train_dataset = TensorDataset(src_data, tgt_data)\n",
        "\n",
        "# Initialize the Transformer model\n",
        "model = Transformer(VOCAB_SIZE, d_model, num_layers, num_heads, d_ff, voices, dropout).to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = CustomLoss(voice_ranges)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
        "criterion = CustomLoss(voice_ranges)\n",
        "\n",
        "def evaluate_model(model):\n",
        "    model.eval()  # Switch to evaluation mode\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():  # Disable gradient computation\n",
        "        for src_seq, tgt_seq in test_loader:\n",
        "            tgt_target = torch.cat((tgt_seq[:, 1:], torch.full_like(tgt_seq[:, :1], PAD_TOKEN_ID)), dim=1)\n",
        "            output = model(src_seq, tgt_seq)\n",
        "            loss = criterion(output, tgt_seq, tgt_target)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint = torch.load(PATH)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4ccONcYWBHV",
        "outputId": "9e8f79ab-d0d2-4b17-8924-cb09ed2f70b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1=============================================\n",
            "Generated-------------------------------------------\n",
            "|99 36 37 36 |37 37 37 37 |39 39 39 39 |41 41 41 41 \n",
            "|32 32 32 32 |32 32 32 32 |32 32 32 32 |32 32 32 32 \n",
            "|27 27 27 27 |29 29 29 25 |24 24 24 24 |25 25 25 25 \n",
            "|20 20 20 18 |17 17 17 17 |20 20 20 20 |13 13 13 13 \n",
            "Target----------------------------------------------\n",
            "|99 36 36 36 |37 37 37 37 |39 39 39 39 |41 41 41 41 \n",
            "|32 32 32 32 |32 32 32 32 |32 32 32 32 |32 32 32 32 \n",
            "|27 27 27 27 |25 25 25 25 |25 25 24 24 |25 25 25 25 \n",
            "|20 20 18 18 |17 17 13 13 |20 20 20 20 |13 13 13 13 \n",
            "Batch [15/120] Loss: 0.8385 | Test Loss: 2.0354145765304565\n",
            "\n",
            "Generated-------------------------------------------\n",
            "|99 41 41 41 |41 41 41 41 |43 39 39 39 |39 36 36 36 \n",
            "|36 36 36 36 |36 34 34 34 |34 34 34 34 |32 32 32 32 \n",
            "|32 32 32 29 |29 29 29 29 |31 31 31 31 |27 20 27 24 \n",
            "|20 20 20 20 |22 22 22 22 |15 15 15 15 |20 20 20 20 \n",
            "Target----------------------------------------------\n",
            "|99 41 41 41 |41 41 41 41 |39 39 39 39 |36 36 36 36 \n",
            "|36 36 36 36 |34 34 34 34 |34 34 34 34 |32 32 32 32 \n",
            "|32 32 31 31 |29 29 29 29 |31 31 31 31 |27 27 24 24 \n",
            "|20 20 20 20 |22 22 22 22 |15 15 15 15 |20 20 20 20 \n",
            "Batch [30/120] Loss: 0.8256 | Test Loss: 2.0173168102900187\n",
            "\n",
            "Generated-------------------------------------------\n",
            "|99 33 33 33 |33 33 33 33 |31 31 33 31 |31 31 31 31 \n",
            "|33 29 29 29 |29 29 29 29 |28 28 28 28 |28 28 28 29 \n",
            "|24 24 24 24 |24 24 24 24 |24 24 22 22 |22 22 12 12 \n",
            "|17 17 17 19 |17 21 29 22 |24 24 26 26 |28 28 14 24 \n",
            "Target----------------------------------------------\n",
            "|99 33 33 33 |33 33 33 33 |31 31 31 31 |31 31 31 31 \n",
            "|29 29 29 29 |29 29 29 29 |28 28 26 26 |28 28 29 29 \n",
            "|24 24 24 24 |24 24 26 26 |28 28 29 29 |31 31 29 29 \n",
            "|17 17 19 19 |21 21 22 22 |24 24 24 24 |12 12 12 12 \n",
            "Batch [45/120] Loss: 0.7335 | Test Loss: 2.038313802083333\n",
            "\n",
            "Generated-------------------------------------------\n",
            "|99 37 37 37 |36 36 36 36 |34 34 34 34 |32 32 32 32 \n",
            "|34 29 29 29 |27 27 27 32 |32 32 32 31 |32 32 32 32 \n",
            "|22 20 20 22 |24 24 27 27 |25 27 27 22 |24 24 24 24 \n",
            "|13 13 17 19 |20 20 20 20 |15 34 15 15 |20 20 20 20 \n",
            "Target----------------------------------------------\n",
            "|99 37 37 37 |36 36 36 36 |34 34 34 34 |32 32 32 32 \n",
            "|29 29 29 29 |27 27 32 32 |32 32 31 31 |32 32 32 32 \n",
            "|20 20 22 22 |24 24 24 25 |27 27 22 22 |24 24 24 24 \n",
            "|17 17 19 19 |20 20 20 20 |27 27 15 15 |20 20 20 20 \n",
            "Batch [60/120] Loss: 0.7176 | Test Loss: 2.0245967547098798\n",
            "\n",
            "Generated-------------------------------------------\n",
            "|99 36 36 36 |36 38 38 38 |39 40 40 40 |41 41 41 41 \n",
            "|34 31 31 31 |29 36 36 36 |34 34 34 34 |36 36 36 36 \n",
            "|27 28 28 28 |32 29 29 29 |22 22 31 31 |32 32 31 32 \n",
            "|22 22 22 22 |20 20 20 20 |19 19 19 19 |17 17 17 17 \n",
            "Target----------------------------------------------\n",
            "|99 36 36 36 |38 38 38 38 |40 40 40 40 |41 41 41 41 \n",
            "|31 31 31 31 |36 36 36 36 |34 34 34 34 |36 36 36 36 \n",
            "|28 28 28 28 |29 29 29 29 |31 31 31 31 |32 32 32 32 \n",
            "|22 22 22 22 |20 20 20 20 |19 19 19 19 |17 17 17 17 \n",
            "Batch [75/120] Loss: 0.7299 | Test Loss: 1.995544195175171\n",
            "\n",
            "Generated-------------------------------------------\n",
            "|99 37 37 37 |36 37 36 35 |34 15 32 34 |32 30 30 32 \n",
            "|34 32 32 30 |29 29 29 29 |25 25 25 25 |25 27 29 29 \n",
            "|29 25 22 25 |20 20 20 20 |22 22 22 22 |23 22 23 23 \n",
            "|22 17 15 15 |13 13 8 13 |13 18 18 17 |15 15 15 15 \n",
            "Target----------------------------------------------\n",
            "|99 37 37 37 |37 37 35 35 |34 34 32 32 |30 30 32 32 \n",
            "|32 32 30 30 |29 29 27 27 |25 25 25 25 |27 27 29 29 \n",
            "|25 25 25 25 |20 20 20 20 |22 22 22 22 |22 22 23 23 \n",
            "|17 17 15 15 |13 13 13 13 |18 18 17 17 |15 15 15 15 \n",
            "Batch [90/120] Loss: 0.7302 | Test Loss: 2.023015030225118\n",
            "\n",
            "Generated-------------------------------------------\n",
            "|99 39 39 39 |41 39 39 39 |41 37 37 37 |39 39 39 39 \n",
            "|34 37 36 37 |36 36 36 36 |32 32 32 32 |32 32 32 32 \n",
            "|32 32 32 32 |32 32 32 30 |29 29 29 29 |27 27 25 27 \n",
            "|20 20 20 20 |20 20 20 21 |13 13 13 13 |24 24 22 22 \n",
            "Target----------------------------------------------\n",
            "|99 39 39 39 |39 39 39 39 |37 37 37 37 |39 39 39 39 \n",
            "|37 37 37 37 |36 36 36 36 |32 32 32 32 |32 32 32 32 \n",
            "|32 32 32 32 |32 32 30 30 |29 29 29 29 |27 27 25 25 \n",
            "|20 20 20 20 |20 20 20 20 |13 13 13 13 |24 24 22 22 \n",
            "Batch [105/120] Loss: 0.6233 | Test Loss: 2.0374531507492066\n",
            "\n",
            "Generated-------------------------------------------\n",
            "|99 31 31 31 |40 31 31 31 |29 29 29 29 |29 29 29 29 \n",
            "|31 28 28 28 |28 28 28 28 |29 24 24 24 |24 24 24 24 \n",
            "|24 24 24 24 |24 22 22 22 |21 21 21 21 |21 21 21 21 \n",
            "|12 12 12 12 |12 12 12 12 |17 17 17 17 |17 17 17 17 \n",
            "Target----------------------------------------------\n",
            "|99 31 31 31 |31 31 31 31 |29 29 29 29 |29 29 29 29 \n",
            "|28 28 28 28 |28 28 28 28 |24 24 24 24 |24 24 24 24 \n",
            "|24 24 24 24 |22 22 22 22 |21 21 21 21 |21 21 21 21 \n",
            "|12 12 12 12 |12 12 12 12 |17 17 17 17 |17 17 17 17 \n",
            "Batch [120/120] Loss: 0.6382 | Test Loss: 2.040420134862264\n",
            "\n",
            "Generated-------------------------------------------\n",
            "|99 41 41 40 |40 40 40 40 |36 40 40 40 |36 40 40 40 \n",
            "|33 38 31 35 |36 36 36 36 |36 36 36 36 |36 36 36 36 \n",
            "|33 31 32 31 |31 31 31 31 |31 31 31 31 |31 31 31 31 \n",
            "|23 23 19 19 |24 24 24 24 |24 24 24 24 |24 24 22 24 \n",
            "Target----------------------------------------------\n",
            "|99 41 40 40 |40 40 40 40 |40 40 40 40 |40 40 40 40 \n",
            "|38 38 35 35 |36 36 36 36 |36 36 36 36 |36 36 36 36 \n",
            "|31 31 31 31 |31 31 31 31 |31 31 31 31 |31 31 31 31 \n",
            "|23 23 19 19 |24 24 24 24 |24 24 24 24 |24 24 24 24 \n",
            "Epoch 1 Loss: 0.7392\n"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.mps.empty_cache() \n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "criterion = CustomLoss(voice_ranges)\n",
        "\n",
        "num_epochs = 1\n",
        "\n",
        "zero_tensor = torch.tensor([99]).to(device)\n",
        "def print_seq(seq, print_len=16):\n",
        "    seq = torch.cat((zero_tensor, seq))\n",
        "    soprano = seq[0::4]\n",
        "    alto = seq[1::4]\n",
        "    tenor = seq[2::4]\n",
        "    bass = seq[3::4]\n",
        "\n",
        "    voices = [soprano, alto, tenor, bass]\n",
        "    for v in voices:\n",
        "        print_str = \"\"\n",
        "        for i, val in enumerate(map(str, v.tolist()[:print_len])):\n",
        "            if i % 4 == 0:\n",
        "                print_str += \"|\"\n",
        "            print_str += val + \" \"\n",
        "        print(print_str)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}\" + \"=\" * 45)\n",
        "    total_loss = 0\n",
        "    num_batches = len(train_loader)\n",
        "    for batch_idx, (src_seq, tgt_seq) in enumerate(train_loader):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        tgt_target = torch.cat((tgt_seq[:, 1:], torch.full_like(tgt_seq[:, :1], PAD_TOKEN_ID)), dim=1)\n",
        "\n",
        "        output = model(src_seq, tgt_seq)\n",
        "\n",
        "        if total_loss == 0:\n",
        "            output_indexes = torch.argmax(output, dim=-1)\n",
        "            print(\"Generated\" + \"-\" * 43)\n",
        "            print_seq(output_indexes[0])\n",
        "            print(\"Target\" + \"-\" * 46)\n",
        "            print_seq(tgt_target[0])\n",
        "\n",
        "        loss = criterion(output, tgt_seq, tgt_target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        print(f\"\\rBatch [{batch_idx + 1}/{num_batches}] Loss: {loss.item():.4f}\", end='', flush=True)\n",
        "\n",
        "        if ((batch_idx+1) % 15 == 0):\n",
        "            print(f\" | Test Loss: {evaluate_model(model)}\")\n",
        "            output_indexes = torch.argmax(output, dim=-1)\n",
        "            print(\"\\nGenerated\" + \"-\" * 43)\n",
        "            print_seq(output_indexes[0])\n",
        "            print(\"Target\" + \"-\" * 46)\n",
        "            print_seq(tgt_target[0])\n",
        "\n",
        "        checkpoint = {\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "        }\n",
        "        torch.save(checkpoint, PATH)\n",
        "\n",
        "        gc.collect()\n",
        "        torch.mps.empty_cache() \n",
        "    \n",
        "    if (epoch+1) % 1 == 0:\n",
        "        print('Epoch {} Loss: {:.4f}'.format(epoch+1, total_loss / len(train_loader)))\n",
        "        # print(f\"Test Loss: {evaluate_model(model)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 2.039167372385661\n"
          ]
        }
      ],
      "source": [
        "print(f\"Test Loss: {evaluate_model(model)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnOIfxVdF9ze",
        "outputId": "78a952bd-b8a9-4593-b89c-cff60ce54bc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating Chorale\n",
            "Generating Scores\n",
            "Exporting Scores\n"
          ]
        }
      ],
      "source": [
        "# hand-evaluation\n",
        "\n",
        "# temp=.5 (.4-.7) best\n",
        "def generate_next_tokens(model, src_seq, tgt_prefix, max_len=320, temperature=.3):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        src_seq = src_seq.unsqueeze(0).to(device)\n",
        "        tgt_seq = tgt_prefix.unsqueeze(0).to(device)\n",
        "        for i in range(max_len):\n",
        "            output = model(src_seq, tgt_seq)\n",
        "            output_last_token = output[:, -1, :]  # Take the last token of the output\n",
        "            # next_token = torch.argmax(output_last_token, dim=-1).unsqueeze(1)\n",
        "            probabilities = torch.nn.functional.softmax(output_last_token / temperature, dim=-1)\n",
        "            next_token = torch.multinomial(probabilities, 1)  # Sample the next token\n",
        "            tgt_seq = torch.cat([tgt_seq, next_token], dim=-1)\n",
        "            if next_token == EOS_TOKEN_ID:  # Add a condition to stop generation\n",
        "                break\n",
        "\n",
        "    return tgt_seq.squeeze(0)\n",
        "\n",
        "def to_score_form(seq, name=\"Unknown\"):\n",
        "    def id_to_note(id):\n",
        "        if id == NO_NOTE_TOKEN_ID or id == PAD_TOKEN_ID or id == EOS_TOKEN_ID:\n",
        "            return 0\n",
        "        return id + NOTE_OFFSET\n",
        "\n",
        "    seq = seq.tolist()\n",
        "    score = [[] for _ in range(4)]\n",
        "    for i in range(len(seq)):\n",
        "        score[i % 4].append(id_to_note(seq[i]))\n",
        "    return [name, score]\n",
        "\n",
        "gc.collect()\n",
        "torch.mps.empty_cache() \n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
        "for src_seq, tgt_seq in test_loader:\n",
        "    print(\"Generating Chorale\")\n",
        "    res = generate_next_tokens(model, src_seq[0], tgt_seq[0][:4])\n",
        "    print(\"Generating Scores\")\n",
        "    score = to_score_form(res, \"Generated Chorale\")\n",
        "    src_score = to_score_form(src_seq[0], \"Source Chorale\")\n",
        "    tgt_score = to_score_form(tgt_seq[0], \"Target Chorale\")\n",
        "    print(\"Exporting Scores\")\n",
        "    visualize_score(src_score, \"6-LossFunctionLarge-Src\")\n",
        "    visualize_score(tgt_score, \"6-LossFunctionLarge-Tgt\")\n",
        "    visualize_score(score, \"6-LossFunctionLarge-Gen\")\n",
        "\n",
        "    gc.collect()\n",
        "    torch.mps.empty_cache() \n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "del model\n",
        "torch.mps.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
